
\chapter{FUNDAMENTAÇÃO TEÓRICA E TRABALHOS RELACIONADOS}\label{Cap:Foundations}

\section{O Vídeo 360°}

O vídeo 360° ou vídeo esférico é chamado assim por se comportar como se estivesse projetado na casca interior de uma esfera onde o usuário espectador está locado no centro e possui liberdade para olhar ao redor, mas não para se mover pelo espaço. O vídeo geralmente é reproduzido em um óculos de realidade virtual (HMD - Head Mounted Display) que atualiza a exibição baseado no movimento da cabeça do espectador, dando a sensação de que o usuário está imerso no vídeo. Como não há movimento de translação no vídeo, geralmente ele é assistido sentado em uma cadeira fixo, como um sofá, por exemplo ou um assento que roda, como uma cadeira de escritório. O vídeo 360° também pode ser reproduzido em navegadores web, como YouTube, onde podemos explorar a esfera do vídeo usando o teclado ou mouse. Além disso, o vídeo 360° pode ser reproduzido em dispositivos moveis celular, onde a exploração da esfera pode ser conectada ao acelerômetro do dispositivo e o usuário mover o celular para visualizar ao redor.



\subsection{A captura}



O HMD
Este trabalho se concentra no vídeo assistido por usuários de um HMD, porém seus resultados podem ser aplicados a qualquer tipo de visualizador. Por conveniência e para dar maior liberdade ao usuário, os HMD devem fazer uso de baterias, o que aumenta seu peso e desconforto e devem se conectar a uma rede sem fio, como 4G ou Wi-Fi.


A criação deste vídeo consiste em quatro etapas que são: captura, costura, projeção e codificação. A saída do codificador pode ser um arquivo para ser reproduzido posteriormente ou transmitido em um streaming ao vivo.

A captura é feita usando duas ou mais lentes do tipo olho de peixe, ou um arranjo com várias câmeras tradicionais. A captura dos quadros nas câmeras precisam ser sincronizadas ou alguma técnica de interpolação pode ser usada no estágio de pós produção. Cada câmera captura uma região da esfera de forma que haja uma significativa sobreposição com as imagens das câmeras vizinhas. Isto significa




O vídeo 360 é capturado usando duas ou mais lentes olho de peixe múltiplas em uma câmera de vídeo VR. Cada lente olho de peixe projeta uma imagem hemisférica (ou quase hemisférica) em um sensor, onde é gravada como um vídeo de origem. Há uma sobreposição significativa nas bordas de cada vídeo de origem e, durante o processo de costura, as seções sobrepostas de cada vídeo são combinadas para produzir um único vídeo esférico 360 em projeção equirretangular.


As câmeras estereoscópicas 3D-360 VR geralmente têm seis ou mais lentes e sensores, e a costura 3D-360 requer uma sobreamostragem completa de todos os pontos do mundo, o que significa que tudo o que a câmera vê deve ser capturado por pelo menos duas lentes adjacentes. Costurar vídeo 3D-360 é muito mais complicado do que costurar 2D-360 mono, e as produções 3D-360 devem considerar a inclusão de um especialista em costura para garantir que a saída seja confortável de visualizar. A saída de um stitcher 3D-360 são duas imagens monoscópicas 360 equirretangulares: uma para cada olho.


A costura monoscópica 2D-360 produz um único vídeo 360 equirretangular. Durante a reprodução, cada olho vê o mesmo vídeo e não há profundidade 3D percebida na experiência.



\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{fig/captura1}
	\caption{legenda aqui}
	\label{fig:captura1}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{fig/registração}
	\caption{legenda aqui}
	\label{fig:registracao}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{fig/eyefish}
	\caption{legenda aqui}
	\label{fig:eyefish}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{fig/esfera}
	\caption{legenda aqui}
	\label{fig:esfera}
\end{figure}


\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{"fig/360 Video - building"}
	\caption{legenda aqui}
	\label{fig:building_360_video}
\end{figure}






\subsection{Projeção}
\url{https://wiki.panotools.org/Cubic_Projection}

\subsubsection{Projeção Cubica Rectilinear}

\url{https://wiki.panotools.org/Panorama_Viewers}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{"fig/projecao_cmp"}
	\caption{legenda aqui}
	\label{fig:projecao_cmp}
\end{figure}

\subsubsection{Projeção Cubica RectilinearProjeção Equirretangular (Projeção cilíndrica equidistante ou projeção de Plate Carré)}

\begin{itemize}
	\item \url{https://www.infoescola.com/cartografia/projecao-cilindrica-equidistante/}
	\item \url{https://en.wikipedia.org/wiki/Equirectangular_projection}
	\item \url{https://pt.wikipedia.org/wiki/Proje%C3%A7%C3%A3o_cil%C3%ADndrica}
	\item \url{https://pt.wikipedia.org/wiki/Proje%C3%A7%C3%A3o_cil%C3%ADndrica_equidistante}
	\item \url{https://docs.qgis.org/3.40/pt_BR/docs/gentle_gis_introduction/coordinate_reference_systems.html}
	\item \url{https://www.fcav.unesp.br/Home/departamentos/engenhariarural/TERESACRISTINATARLEPISSARRA/edital.pdf}
	\item \url{https://brasilescola.uol.com.br/geografia/projecoes-cartograficas.htm}
	\item \url{https://www.infoescola.com/cartografia/projecao-cilindrica-equidistante/}
	\item \url{https://proj.org/en/stable/operations/projections/eqc.html}
\end{itemize}

Os prints abaixo são desse livro

\url{https://www.google.com.br/books/edition/Flattening_the_Earth/0UzjTJ4w9yEC?hl=pt-BR&gbpv=1&dq=isbn:0226767477&printsec=frontcover}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{"fig/screenshot_livro1"}
	\caption{legenda aqui}
	\label{fig:screenshot_livro1}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{"fig/screenshot_livro2"}
	\caption{legenda aqui}
	\label{fig:screenshot_livro2}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{"fig/tissot"}
	\caption{Indicador de tissot para a projeção cubemap}
	\label{fig:tissot}
\end{figure}

\subsection{Outras projeções}

\begin{itemize}
	\item Octahedron
	\item Truncated Square Pyramid
	\item Fisheye
	\item Hybrid equi-angular cubemap
	\item Equi-angular cubemap
	\item Equatorial cylindrical projection
	\item Segmented Sphere Projection
	\item Icosahedron
	\item …
	\item \url{https://jvet.hhi.fraunhofer.de/svn/svn_360Lib/tags/HM-16.9-360Lib-1.0-rc1/}
\end{itemize}

\section{Transmissão de vídeos 360 com ladrilhos}

O streaming de vídeo 360 envolve várias etapas de processamento de imagens, como mostra a figura XXXX. O processo começa com a captura do vídeo através de arranjos de câmeras tradicionais. As imagens precisam ser processadas, coladas e então projetadas em uma superfície plana onde um codificador de vídeo como H.265 ou AV1 é usado para comprimi-lo. Como o campo visual do ser humano é limitado, o usuário encherga apenas uma fração de toda as esfera. Assim, para que o que é visto tenha uma boa resolução, como Full DH, a projeção deve ter uma resolução muito maior, como 4K ou até mesmo 12K. Porém, como processar partes da esfera que não são assistidas desperdiçam recursos, a projeção é segmentada espacialmente em ladrilhos (tiles) que possam ser decodificados forma independentes.

A criação do streaming de vídeo esférico envolve várias etapas envolvendo diversas técnicas de processamento de imagem para

Captura -> stitch --> projeção --> tiling --> codificação -->  Dashing --> Streaming --> decoding --> mount --> Rectilinear Projection (Viewport) --> display
\subsection{Tiling}
\subsection{codificação}
\subsubsection{O vídeo plano - Camada de codificação de vídeo (VLC)}

Colocar imagem da VLC mostrando a ordem do processamento intrapreditivo (quadros I) e interpreditivo (Quadros P e B).

Características da codificação:

\begin{itemize}
	\item Spatial
	\begin{itemize}
		\item Color Space YUV. Color Subsampling (4:4:4, 4:2:0, ...)
		\item Macroblock: 8x8, 16x16, ...
		\item Discrete Cosine Transform (DCT)
		\item Quantization function
		\item Arithmetic Entropy Coding
	\end{itemize}
	\item Temporal
	\begin{itemize}
		\item Motion Estimator (computes motion vector)
		\item Motion Compensator
	\end{itemize}
\end{itemize}

Depois, falar sobre o GOP e a disposição dos quadros dentro do gop. Falar sobre o GOP aberto e o GOP fechado.

Enfatizar a dificuldade de decodificar um quadro específico pois a ordem de decodificação não é a mesma da ordem de reprodução.

Falar sobre codificação com apenas quadros I e sem o quadro B (Fast Decoding).

\url{http://ip.hhi.de/imagecom_G1/assets/pdfs/csvt_overview_0305.pdf }

\url{http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf}

\subsubsection{Bitstream Structure - Network Abstraction Layer}

\begin{itemize}
    \item NAL Units (h.264 = 21, H.265 = 41)
    \begin{itemize}
        \item Organize Bitstream
        \item Header = 1 byte
    \end{itemize}
    \item VCL and non-VCL NAL Units
    \item Used in
    \begin{itemize}
        \item Packet-oriented transport (IP/RTP)
        \item Bitstream-oriented transport (H.320 and MPEG-2/H.222.0). Have start code.
    \end{itemize}

\end{itemize}

mostrar imagem daquele artigo sobre overview of high order abstration layer of hevc (NAL)


\subsubsection{ISObmff boxes and the CMAF}

mostrar a hierarquia do ISOBMFF. tem uma imagem de uma árvore no documento do padrão. Tem junto uma lista com os poxes. Fala sobre a fragmentação dos pacotes NAL Unit para sincronização entre mídias e permitir a busca rápida.

\subsubsection{Media container (MP4 Format)}

•	•Extends ISO Base Media File Format (ISOBMFF)

•	•Organize media streams in “tracks”

•	•Carriage of NAL unit

•	•Basic structure is called Box

Colocar aquela imagem que mostra o moov e mdata do container MP4. Ver no documento do site chiaglione.

\url{https://mpeg.chiariglione.org/standards/mpeg-4}




\subsection{Dashing - HAS (HTTP Adaptive Streaming)}

M. Hosseini and V. Swaminathan, “Adaptive 360 VR video streaming: Divide and conquer,” in Proc. IEEE Int. Symp. Multimedia (ISM), San Jose, CA, USA, Dec. 2016, pp. 107–110.

M. Graf, C. Timmerer, and C. Mueller, “Towards bandwidth efficient adaptive streaming of omnidirectional video over HTTP: Design, implementation, and evaluation,” in Proc. 8th ACM Multimedia Syst. Conf. (MMSys), Taipei, Taiwan, Jun. 2017, pp. 261–271.

•A. Zare, A. Aminlou, M. M. Hannuksela, and M. Gabbouj, “HEVC- compliant tile-based streaming of panoramic video for virtual reality applications,” in Proc. IEEE Picture Coding Symp. (PCS), Nuremberg, Germany, Dec. 2016, pp. 601–605.

•R. Skupin, Y. Sanchez, D. Podborski, C. Hellge, and T. Schierl, “HEVC tile based streaming to head mounted displays,” in Proc. 14th IEEE Annu. Consum. Commun. Netw. Conf. (CCNC),Las Vegas, NV, USA, Jan. 2017, pp. 613–615.

•S. Petrangeli, V. Swaminathan, M. Hosseini, and F. De Turck, “An HTTP/2-based adaptive streaming framework for 360◦ virtual reality videos,” in Proc. ACM Multimedia Conf., Mountain View, CA, USA, Oct. 2017, pp. 1–9

•C. Ozcinar, A. De Abreu, and A. Smolic, “Viewport-aware adap- tive 360◦ video streaming using tiles for virtual reality,” in Proc. IEEE Int. Conf. Image Process. (ICIP), Beijing, China, Sep. 2017, pp. 2174–2178.

•P. R. Alface, J.-F. Macq, and N. Verzijp, “Interactive omnidirectional video delivery: A bandwidth-effective approach,” Bell Labs Tech. J., vol. 16, no. 4, pp. 135–148, 2012.

•L. Xie, Z. Xu, Y. Ban, X. Zhang, and Z. Guo, “360ProbDASH: Improving QoE of 360 video streaming using tile-based HTTP adaptive streaming,” in Proc. ACM Multimedia Conf., Mountain View, CA, USA, Oct. 2017, pp. 315–323

•A. T. Nasrabadi, A. Mahzari, J. D. Beshay, and R. Prakash, “Adaptive 360-degree video streaming using scalable video coding,” in Proc. ACM Multimedia Conf., Mountain View, CA, USA, Oct. 2017, pp. 1689–1697.

•Y. Sanchez, R. Skupin, C. Hellge, and T. Schierl, “Random access point period optimization for viewport adaptive tile based streaming of 360◦ video,” in Proc. IEEE Int. Conf. Image Process. (ICIP), Beijing, China, Sep. 2017, pp. 1915–1919.

•M. Xiao, C. Zhou, Y. Liu, and S. Chen, “OpTile: Toward optimal tiling in 360-degree video streaming,” in Proc. ACM Multimedia Conf. Mountain View, CA, USA, Oct. 2017, pp. 708–716.

•Z. Tu et al., “Content adaptive tiling method based on user access preference for streaming panoramic video,” in Proc. IEEE Int. Conf. Consum. Electron. (ICCE), Las Vegas, NV, USA, Jan. 2018, pp. 1–4.

•F. Qian, B. Han, Q. Xiao, e V. Gopalakrishnan, “Flare: Practical Viewport-Adaptive 360-Degree Video Streaming for Mobile Devices”, in Proceedings of the 24th Annual International Conference on Mobile Computing and Networking - MobiCom ’18, 2018, no Ml, p. 99–114.

\subsubsection{DASH-SRD}
O DASH SRD é um admendum do padrão DASH ISO-XXXXX, que inclui a capacidade de mesclar diversos vídeos em único quadro com transmissão adaptativa independente. Assim, seria possível transmitir vários vídeos e imagens de forma independente e o cliente controlava a qualidade e taxa de bits de acordo com a largura de banda disponível. Caso a largura de banda disponível oscilasse, a qualidade poderia aumentar ou diminuir dinamicamente.

Uma vez que o ser humano não consegue enxergar toda a esfera do vídeo, podemos segmentar a projeção da esfera em ladrilhos, onde cada ladrilho compreenderia um vídeo independente e assim, transmitir apenas os ladrilhos na posição que estão sendo vistos. Apesar desta abordagem reduzir a eficiência do codificador, os ganhos são maiores.

Para isto, o servidor deve segmentar o vídeo em CMP em ladrilhos, então cada ladrilho é codificado com várias qualidades diferentes, o que produz vídeos com diferentes taxas de bits. Em seguida os vídeos são segmentados em chunks de mesma duração e disponibilizados em um servidor HTTP.

Para que cada chunk possa ser decodificado independentemente, um arquivo apenas com o cabeçalho é disponibilizado na inicialização do vídeo. As URLs dos chunks de todos os ladrilhos, suas taxas de bits médias, a posição de cada streaming do quadro e outras informações como codificador, taxa de quadros, e outras anotações, são estruturados em um arquivo XML chamado MPD. Ao iniciar o vídeo, o primeiro arquivo solicitado pelo player é o MPD. Com estas informações o cliente requisita os chunks de cada streaming na qualidade que melhor se adequa a largura de banda disponível.

\begin{itemize}
       \item Others: Apple HLS, Microsoft Smooth Streaming, Adobe ADS
       \item •Open-Source
       \item Codec Agnostic
       \item Quality == Bit Rate (size * 8 / duration
       \item Chunks à Constant Duration
       \item Change quality after full chunk download
       \item Chunk start on I frame
       \item Fixed size tiles
       \item GPAC (\url{https://gpac.wp.imt.fr/})
       \item https://github.com/gpac/gpac
       \item MPEG DASH SRD: spatial relationship description.
       \item \url{https://dl.acm.org/doi/10.1145/2910017.2910606}
       \item \url{https://github.com/gpac/gpac/wiki/Tiled-Streaming}
       \item
       \item
       \item


\end{itemize}


•Adaptive 360 VR Video Streaming based on MPEG-DASH SRD. https://arxiv.org/abs/1701.06509

•Tiled panoramic video transmission system based on MPEG-DASH. https://ieeexplore.ieee.org/document/7354646/

•https://github.com/gpac/gpac/wiki/HEVC-Tile-based-adaptation-guide

•https://github.com/gpac/gpac/wiki/MPEG-DASH-SRD-and-HEVC-tiling-for-VR-videos


Agora mostrar uma figura do padrão do DASH SRD. procurar aqueles artigos que falam do dash srd e ver na documentação dopadrão

\subsubsection{MPD}

Agora falar sobre o MPD e colocar as confgurações dele.

\subsubsection{ABR}

''Algoritmos baseados em regras e aprendizado de máquina têm sido amplamente utilizados para adaptação de taxa de bits em vídeos 360° [3, 4].``

\subsection{Viewport}

Projeção gnomônica

\url{https://en.wikipedia.org/wiki/Gnomonic_projection}

''The gnomonic projection is used extensively in photography, where it is called rectilinear projection, as it naturally arises from the pinhole camera model where the screen is a plane.[3] Because they are equivalent, the same viewer used for photographic panoramas can be used to render gnomonic maps (view as a 360° interactive panorama).`` - Wikipédia

\subsubsection{The Head Mounted Display and the Body Coordinate System}

•	Accelerometer and Compass

•	Euler Rotation (ZàYàX)

\subsubsection{Predição de viewport e erro de predição}

Optimizing 360 Video Delivery Over Cellular Networks
\url{https://dl.acm.org/doi/10.1145/2980055.2980056}

•Viewport Prediction for 360◦ Videos: A Clustering Approach
https://dl.acm.org/doi/10.1145/3386290.3396934

•Revisiting Deep Architectures for Head Motion Prediction in 360° Videos
https://arxiv.org/abs/1911.11702


\subsection{Limitações da Literatura}
\subsection{Conexão como trabalho proposto}
\subsection{Avaliação de Desempenho}


\section{360EAVP}
\subsection{Preparação}
\subsection{Identificador de face}
\subsection{Preditor de Viewport}
\subsection{Sobre a estrutura do programa}

\section{Modelagem de QoE}
