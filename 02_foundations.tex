
\chapter{FUNDAMENTAÇÃO TEÓRICA E TRABALHOS RELACIONADOS}\label{Cap:Foundations}

\section{O Vídeo 360°}

O vídeo 360° ou vídeo esférico é chamado assim por se comportar como se estivesse projetado na casca interior de uma esfera onde o usuário espectador está locado no centro e possui liberdade para olhar ao redor, mas não para se mover pelo espaço. O vídeo geralmente é reproduzido em um óculos de realidade virtual (HMD - Head Mounted Display) que atualiza a exibição baseado no movimento da cabeça do espectador, dando a sensação de que o usuário está imerso no vídeo. Como não há movimento de translação no vídeo, geralmente ele é assistido sentado em uma cadeira fixo, como um sofá, por exemplo ou um assento que roda, como uma cadeira de escritório. O vídeo 360° também pode ser reproduzido em navegadores web, como YouTube, onde podemos explorar a esfera do vídeo usando o teclado ou mouse. Além disso, o vídeo 360° pode ser reproduzido em dispositivos moveis celular, onde a exploração da esfera pode ser conectada ao acelerômetro do dispositivo e o usuário mover o celular para visualizar ao redor.



\subsection{A captura}



O HMD
Este trabalho se concentra no vídeo assistido por usuários de um HMD, porém seus resultados podem ser aplicados a qualquer tipo de visualizador. Por conveniência e para dar maior liberdade ao usuário, os HMD devem fazer uso de baterias, o que aumenta seu peso e desconforto e devem se conectar a uma rede sem fio, como 4G ou Wi-Fi.


A criação deste vídeo consiste em quatro etapas que são: captura, costura, projeção e codificação. A saída do codificador pode ser um arquivo para ser reproduzido posteriormente ou transmitido em um streaming ao vivo.

A captura é feita usando duas ou mais lentes do tipo olho de peixe, ou um arranjo com várias câmeras tradicionais. A captura dos quadros nas câmeras precisam ser sincronizadas ou alguma técnica de interpolação pode ser usada no estágio de pós produção. Cada câmera captura uma região da esfera de forma que haja uma significativa sobreposição com as imagens das câmeras vizinhas. Isto significa




O vídeo 360 é capturado usando duas ou mais lentes olho de peixe múltiplas em uma câmera de vídeo VR. Cada lente olho de peixe projeta uma imagem hemisférica (ou quase hemisférica) em um sensor, onde é gravada como um vídeo de origem. Há uma sobreposição significativa nas bordas de cada vídeo de origem e, durante o processo de costura, as seções sobrepostas de cada vídeo são combinadas para produzir um único vídeo esférico 360 em projeção equirretangular.


As câmeras estereoscópicas 3D-360 VR geralmente têm seis ou mais lentes e sensores, e a costura 3D-360 requer uma sobreamostragem completa de todos os pontos do mundo, o que significa que tudo o que a câmera vê deve ser capturado por pelo menos duas lentes adjacentes. Costurar vídeo 3D-360 é muito mais complicado do que costurar 2D-360 mono, e as produções 3D-360 devem considerar a inclusão de um especialista em costura para garantir que a saída seja confortável de visualizar. A saída de um stitcher 3D-360 são duas imagens monoscópicas 360 equirretangulares: uma para cada olho.


A costura monoscópica 2D-360 produz um único vídeo 360 equirretangular. Durante a reprodução, cada olho vê o mesmo vídeo e não há profundidade 3D percebida na experiência.



\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{fig/captura1}
	\caption{legenda aqui}
	\label{fig:captura1}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{fig/registração}
	\caption{legenda aqui}
	\label{fig:registracao}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{fig/eyefish}
	\caption{legenda aqui}
	\label{fig:eyefish}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{fig/esfera}
	\caption{legenda aqui}
	\label{fig:esfera}
\end{figure}


\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{"fig/360 Video - building"}
	\caption{legenda aqui}
	\label{fig:building_360_video}
\end{figure}






\subsection{Projeção}
\url{https://wiki.panotools.org/Cubic_Projection}

\subsubsection{Projeção Cubica Rectilinear}

\url{https://wiki.panotools.org/Panorama_Viewers}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{"fig/projecao_cmp"}
	\caption{legenda aqui}
	\label{fig:projecao_cmp}
\end{figure}

\subsubsection{Projeção Cubica RectilinearProjeção Equirretangular (Projeção cilíndrica equidistante ou projeção de Plate Carré)}

\begin{itemize}
	\item \url{https://www.infoescola.com/cartografia/projecao-cilindrica-equidistante/}
	\item \url{https://en.wikipedia.org/wiki/Equirectangular_projection}
	\item \url{https://pt.wikipedia.org/wiki/Proje%C3%A7%C3%A3o_cil%C3%ADndrica}
	\item \url{https://pt.wikipedia.org/wiki/Proje%C3%A7%C3%A3o_cil%C3%ADndrica_equidistante}
	\item \url{https://docs.qgis.org/3.40/pt_BR/docs/gentle_gis_introduction/coordinate_reference_systems.html}
	\item \url{https://www.fcav.unesp.br/Home/departamentos/engenhariarural/TERESACRISTINATARLEPISSARRA/edital.pdf}
	\item \url{https://brasilescola.uol.com.br/geografia/projecoes-cartograficas.htm}
	\item \url{https://www.infoescola.com/cartografia/projecao-cilindrica-equidistante/}
	\item \url{https://proj.org/en/stable/operations/projections/eqc.html}
\end{itemize}

Os prints abaixo são desse livro

\url{https://www.google.com.br/books/edition/Flattening_the_Earth/0UzjTJ4w9yEC?hl=pt-BR&gbpv=1&dq=isbn:0226767477&printsec=frontcover}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{"fig/screenshot_livro1"}
	\caption{legenda aqui}
	\label{fig:screenshot_livro1}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{"fig/screenshot_livro2"}
	\caption{legenda aqui}
	\label{fig:screenshot_livro2}
\end{figure}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.1\linewidth]{"fig/tissot"}
	\caption{Indicador de tissot para a projeção cubemap}
	\label{fig:tissot}
\end{figure}

\subsection{Outras projeções}

\begin{itemize}
	\item Octahedron
	\item Truncated Square Pyramid
	\item Fisheye
	\item Hybrid equi-angular cubemap
	\item Equi-angular cubemap
	\item Equatorial cylindrical projection
	\item Segmented Sphere Projection
	\item Icosahedron
	\item …
	\item \url{https://jvet.hhi.fraunhofer.de/svn/svn_360Lib/tags/HM-16.9-360Lib-1.0-rc1/}
\end{itemize}

\section{Transmissão de vídeos 360 com ladrilhos}

O streaming de vídeo 360 envolve várias etapas de processamento de imagens, como mostra a figura XXXX. O processo começa com a captura do vídeo através de arranjos de câmeras tradicionais. As imagens precisam ser processadas, coladas e então projetadas em uma superfície plana onde um codificador de vídeo como H.265 ou AV1 é usado para comprimi-lo. Como o campo visual do ser humano é limitado, o usuário enxerga apenas uma fração de toda as esfera. Assim, para que o que é visto tenha uma boa resolução, como FullHD, a projeção deve ter uma resolução muito maior, como 4K ou até mesmo 12K. Porém, como processar partes da esfera que não são assistidas desperdiçam recursos, a projeção é segmentada espacialmente em ladrilhos (tiles) que possam ser decodificados forma independentes.

A criação do streaming de vídeo esférico envolve várias etapas envolvendo diversas técnicas de processamento de imagem para

Captura -> stitch --> projeção --> tiling --> codificação -->  Dashing --> Streaming --> decoding --> mount --> Rectilinear Projection (Viewport) --> display

\subsection{Tiling}
aqui falar sobre os autores que propoe tiles de tamanho diferentes e tiles pré recortados

\subsection{codificação}

\subsubsection{O vídeo plano - Camada de codificação de vídeo (VLC)}

Colocar imagem da VLC mostrando a ordem do processamento intrapreditivo (quadros I) e interpreditivo (Quadros P e B).

Características da codificação:

\begin{itemize}
	\item Spatial
	\begin{itemize}
		\item Color Space YUV. Color Subsampling (4:4:4, 4:2:0, ...)
		\item Macroblock: 8x8, 16x16, ...
		\item Discrete Cosine Transform (DCT)
		\item Quantization function
		\item Arithmetic Entropy Coding
	\end{itemize}
	\item Temporal
	\begin{itemize}
		\item Motion Estimator (computes motion vector)
		\item Motion Compensator
	\end{itemize}
\end{itemize}

Depois, falar sobre o GOP e a disposição dos quadros dentro do gop. Falar sobre o GOP aberto e o GOP fechado.

Enfatizar a dificuldade de decodificar um quadro específico pois a ordem de decodificação não é a mesma da ordem de reprodução.

Falar sobre codificação com apenas quadros I e sem o quadro B (Fast Decoding).

\url{http://ip.hhi.de/imagecom_G1/assets/pdfs/csvt_overview_0305.pdf }

\url{http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf}

\subsubsection{Bitstream Structure - Network Abstraction Layer}

\begin{itemize}
    \item NAL Units (h.264 = 21, H.265 = 41)
    \begin{itemize}
        \item Organize Bitstream
        \item Header = 1 byte
    \end{itemize}
    \item VCL and non-VCL NAL Units
    \item Used in
    \begin{itemize}
        \item Packet-oriented transport (IP/RTP)
        \item Bitstream-oriented transport (H.320 and MPEG-2/H.222.0). Have start code.
    \end{itemize}

\end{itemize}

mostrar imagem daquele artigo sobre overview of high order abstration layer of hevc (NAL)


\subsubsection{ISObmff boxes and the CMAF}

mostrar a hierarquia do ISOBMFF. tem uma imagem de uma árvore no documento do padrão. Tem junto uma lista com os poxes. Fala sobre a fragmentação dos pacotes NAL Unit para sincronização entre mídias e permitir a busca rápida.

\subsubsection{Media container (MP4 Format)}

•	•Extends ISO Base Media File Format (ISOBMFF)

•	•Organize media streams in “tracks”

•	•Carriage of NAL unit

•	•Basic structure is called Box

Colocar aquela imagem que mostra o moov e mdata do container MP4. Ver no documento do site chiaglione.

\url{https://mpeg.chiariglione.org/standards/mpeg-4}



\subsection{Dashing - HAS (HTTP Adaptive Streaming)}

M. Hosseini and V. Swaminathan, “Adaptive 360 VR video streaming: Divide and conquer,” in Proc. IEEE Int. Symp. Multimedia (ISM), San Jose, CA, USA, Dec. 2016, pp. 107–110.

M. Graf, C. Timmerer, and C. Mueller, “Towards bandwidth efficient adaptive streaming of omnidirectional video over HTTP: Design, implementation, and evaluation,” in Proc. 8th ACM Multimedia Syst. Conf. (MMSys), Taipei, Taiwan, Jun. 2017, pp. 261–271.

•A. Zare, A. Aminlou, M. M. Hannuksela, and M. Gabbouj, “HEVC- compliant tile-based streaming of panoramic video for virtual reality applications,” in Proc. IEEE Picture Coding Symp. (PCS), Nuremberg, Germany, Dec. 2016, pp. 601–605.

•R. Skupin, Y. Sanchez, D. Podborski, C. Hellge, and T. Schierl, “HEVC tile based streaming to head mounted displays,” in Proc. 14th IEEE Annu. Consum. Commun. Netw. Conf. (CCNC),Las Vegas, NV, USA, Jan. 2017, pp. 613–615.

•S. Petrangeli, V. Swaminathan, M. Hosseini, and F. De Turck, “An HTTP/2-based adaptive streaming framework for 360◦ virtual reality videos,” in Proc. ACM Multimedia Conf., Mountain View, CA, USA, Oct. 2017, pp. 1–9

•C. Ozcinar, A. De Abreu, and A. Smolic, “Viewport-aware adap- tive 360◦ video streaming using tiles for virtual reality,” in Proc. IEEE Int. Conf. Image Process. (ICIP), Beijing, China, Sep. 2017, pp. 2174–2178.

•P. R. Alface, J.-F. Macq, and N. Verzijp, “Interactive omnidirectional video delivery: A bandwidth-effective approach,” Bell Labs Tech. J., vol. 16, no. 4, pp. 135–148, 2012.

•L. Xie, Z. Xu, Y. Ban, X. Zhang, and Z. Guo, “360ProbDASH: Improving QoE of 360 video streaming using tile-based HTTP adaptive streaming,” in Proc. ACM Multimedia Conf., Mountain View, CA, USA, Oct. 2017, pp. 315–323

•A. T. Nasrabadi, A. Mahzari, J. D. Beshay, and R. Prakash, “Adaptive 360-degree video streaming using scalable video coding,” in Proc. ACM Multimedia Conf., Mountain View, CA, USA, Oct. 2017, pp. 1689–1697.

•Y. Sanchez, R. Skupin, C. Hellge, and T. Schierl, “Random access point period optimization for viewport adaptive tile based streaming of 360◦ video,” in Proc. IEEE Int. Conf. Image Process. (ICIP), Beijing, China, Sep. 2017, pp. 1915–1919.

•M. Xiao, C. Zhou, Y. Liu, and S. Chen, “OpTile: Toward optimal tiling in 360-degree video streaming,” in Proc. ACM Multimedia Conf. Mountain View, CA, USA, Oct. 2017, pp. 708–716.

•Z. Tu et al., “Content adaptive tiling method based on user access preference for streaming panoramic video,” in Proc. IEEE Int. Conf. Consum. Electron. (ICCE), Las Vegas, NV, USA, Jan. 2018, pp. 1–4.

•F. Qian, B. Han, Q. Xiao, e V. Gopalakrishnan, “Flare: Practical Viewport-Adaptive 360-Degree Video Streaming for Mobile Devices”, in Proceedings of the 24th Annual International Conference on Mobile Computing and Networking - MobiCom ’18, 2018, no Ml, p. 99–114.

\subsubsection{DASH-SRD}
O DASH SRD é um admendum do padrão DASH ISO-XXXXX, que inclui a capacidade de mesclar diversos vídeos em único quadro com transmissão adaptativa independente. Assim, seria possível transmitir vários vídeos e imagens de forma independente e o cliente controlava a qualidade e taxa de bits de acordo com a largura de banda disponível. Caso a largura de banda disponível oscilasse, a qualidade poderia aumentar ou diminuir dinamicamente.

Uma vez que o ser humano não consegue enxergar toda a esfera do vídeo, podemos segmentar a projeção da esfera em ladrilhos, onde cada ladrilho compreenderia um vídeo independente e assim, transmitir apenas os ladrilhos na posição que estão sendo vistos. Apesar desta abordagem reduzir a eficiência do codificador, os ganhos são maiores.

Para isto, o servidor deve segmentar o vídeo em CMP em ladrilhos, então cada ladrilho é codificado com várias qualidades diferentes, o que produz vídeos com diferentes taxas de bits. Em seguida os vídeos são segmentados em chunks de mesma duração e disponibilizados em um servidor HTTP.

Para que cada chunk possa ser decodificado independentemente, um arquivo apenas com o cabeçalho é disponibilizado na inicialização do vídeo. As URLs dos chunks de todos os ladrilhos, suas taxas de bits médias, a posição de cada streaming do quadro e outras informações como codificador, taxa de quadros, e outras anotações, são estruturados em um arquivo XML chamado MPD. Ao iniciar o vídeo, o primeiro arquivo solicitado pelo player é o MPD. Com estas informações o cliente requisita os chunks de cada streaming na qualidade que melhor se adequa a largura de banda disponível.

\begin{itemize}
       \item Others: Apple HLS, Microsoft Smooth Streaming, Adobe ADS
       \item •Open-Source
       \item Codec Agnostic
       \item Quality == Bit Rate (size * 8 / duration
       \item Chunks à Constant Duration
       \item Change quality after full chunk download
       \item Chunk start on I frame
       \item Fixed size tiles
       \item GPAC (\url{https://gpac.wp.imt.fr/})
       \item https://github.com/gpac/gpac
       \item MPEG DASH SRD: spatial relationship description.
       \item \url{https://dl.acm.org/doi/10.1145/2910017.2910606}
       \item \url{https://github.com/gpac/gpac/wiki/Tiled-Streaming}
       \item
       \item
       \item


\end{itemize}


•Adaptive 360 VR Video Streaming based on MPEG-DASH SRD. \url{https://arxiv.org/abs/1701.06509}

•Tiled panoramic video transmission system based on MPEG-DASH. \url{https://ieeexplore.ieee.org/document/7354646/}

•\url{https://github.com/gpac/gpac/wiki/HEVC-Tile-based-adaptation-guide}

•\url{https://github.com/gpac/gpac/wiki/MPEG-DASH-SRD-and-HEVC-tiling-for-VR-videos}


Agora mostrar uma figura do padrão do DASH SRD. procurar aqueles artigos que falam do dash srd e ver na documentação dopadrão

\subsubsection{MPD}

Agora falar sobre o MPD e colocar as confgurações dele.

\subsubsection{ABR}

''Algoritmos baseados em regras e aprendizado de máquina têm sido amplamente utilizados para adaptação de taxa de bits em vídeos 360° [3, 4].``

\subsection{Viewport}

Projeção gnomônica

\url{https://en.wikipedia.org/wiki/Gnomonic_projection}

''The gnomonic projection is used extensively in photography, where it is called rectilinear projection, as it naturally arises from the pinhole camera model where the screen is a plane.[3] Because they are equivalent, the same viewer used for photographic panoramas can be used to render gnomonic maps (view as a 360° interactive panorama).`` - Wikipédia

\subsubsection{The Head Mounted Display and the Body Coordinate System}

•	Accelerometer and Compass

•	Euler Rotation (ZàYàX)

\subsubsection{Predição de viewport e erro de predição}

Optimizing 360 Video Delivery Over Cellular Networks
\url{https://dl.acm.org/doi/10.1145/2980055.2980056}

•Viewport Prediction for 360◦ Videos: A Clustering Approach
\url{https://dl.acm.org/doi/10.1145/3386290.3396934}

•Revisiting Deep Architectures for Head Motion Prediction in 360° Videos
\url{https://arxiv.org/abs/1911.11702}

\subsubsection{Métodos de seleção de ladrilhos}

An Evaluation of Tile Selection Methods for Viewport-Adaptive Streaming of 360-Degree Video. \url{https://dl.acm.org/doi/10.1145/3373359}

\subsubsection{About Quality}

Tem aquele estudo do MPEG que fala sobre qualidade e depois um outro estudo falando que o S-PSNR não é muito bom.

•PSNR

•S-PSNR

•WS-PSNR

•S-SSIM

IV-PSNR

•A Study on Quality Metrics for 360 Video Communications. 2018 IEICE TRANSACTIONS on Information and Systems.

•Huyen T. T. Tran. Postdoctoral Researcher, University of Aizu. \url{https://github.com/TranHuyen1191}

\url{https://scholar.google.com/citations?hl=en&user=EmQjFBcAAAAJ&view_op=list_works&sortby=pubdate}

•An Evaluation of Tile Selection Methods for Viewport-Adaptive Streaming of 360-Degree Video. 2020 ACM Transactions on Multimedia Computing, Communications, and Applications. 

•An Optimal Tile-Based Approach for Viewport-Adaptive 360-Degree Video Streaming.  IEEE Journal on Emerging and Selected Topics in Circuits and Systems ( Volume: 9, Issue: 1, March 2019)

•A New Adaptation Approach for Viewport-adaptive 360-degree Video Streaming. 2017 IEEE International Symposium on Multimedia (ISM).

•A Weighted Viewport Quality Metric for Omnidirectional Images. 2020 IEICE TRANSACTIONS on Information and Systems   Vol.E103-D   No.1   pp.67-70

•Non-reference Quality Assessment Model using Deep learning for Omnidirectional Images.  2019 IEEE 10th International Conference on Awareness Science and Technology (iCAST)

•Towards an Overall QoE Model for 360-Degree Video. 2020 IEEE Eighth International Conference on Communications and Electronics (ICCE)

•A Subjective Study on User Perception Aspects in Virtual Reality. Applied Sciences (MDPI). 2019; 9(16):3384.

•A Cumulative Quality Model for HTTP Adaptive Streaming. 2018 Tenth International Conference on Quality of Multimedia Experience (QoMEX)

•Impact of delays on 360-degree video communications. 2017 TRON Symposium (TRONSHOW).



\subsection{Limitações da Literatura}

Resuma as limitações dos estudos existentes:

''A ausência de um framework unificado para avaliação dificulta a comparação de algoritmos ABR e a validação de propostas em cenários realistas.``


\subsection{Conexão como trabalho proposto}

Explique como seu trabalho resolve essas limitações:

''Este artigo aborda essas lacunas ao propor uma metodologia unificada para avaliação de algoritmos ABR, considerando múltiplos elementos sincronizados e métricas consistentes.``


\subsection{Avaliação de Desempenho}

Discuta as metodologias existentes para avaliação:

''A avaliação de algoritmos ABR para vídeos 360° geralmente utiliza métricas como PSNR, SSIM e MSE, mas as abordagens variam amplamente entre os estudos [5, 6].``

Destaque a lacuna de uma metodologia padronizada para múltiplos elementos:

''Embora algumas propostas considerem a segmentação espacial, poucas abordam o impacto de múltiplos elementos sincronizados com adaptação independente de taxa de bits.``


\section{360EAVP}

O 360EAVP, diferentemente dos outros tocadores, constrói um cubo virtual em vez de uma esfera. Em cada face do cubo é exibida os ladrilhos de uma face da projeção CUBEMAP de forma que o usuário não perceba a diferença. Desta forma o player obriga que as faces da projeção cubemap sejam transmitidas separadamente e cada face poderá ser segmentada em qualquer formato de ladrilhos. O padrão de ladrilhamento 3x2 cada face do cubo possui apenas um ladrilho. No padrão 6x4 cada face é segmentada no padrão 2x2, no padrão 9x6 cada face é segmentada no formato 3x3 e no padrão 12x8 cada face é segmentada no padrão 4x4.

\begin{tabular}{|c|c|}
	\hline
	Segmentação
	da projeção
	& Segmentação
	da face
	\\
	\hline
	3x2 & 1x1 \\
	\hline
	6x4 & 2x2 \\
	\hline
	9x6 & 3x3 \\
	\hline
	12x8 & 4x4 \\
	\hline
\end{tabular}

Para cada ladrilho é instanciado um player independente que recebe a URL do MPD deste ladrilho. Cada MPD contem as informações sobre taxa de bits de cada representação de qualidade do vídeo e informações de como decodificar cada chunk.

O 360EAVP inclui os seguintes módulos:

\begin{itemize}
	\item Módulo de preparação;
	\item Módulo de edição dinâmica;
	\item Módulo de identificação de faces visíveis;
	\item Módulo de previsão de viewport;
	\item Módulo ABR consciente da edição;
	\item Módulo de rastreamento de dados de reprodução de vídeo
\end{itemize}

colocar uma imagem do trabalho do gabriel. Figura 4.1

\subsection{Preparação}

colocar a figura 4.2

\begin{itemize}
	\item 
	\item Solicitar o JSON de vídeo do servidor, permitindo que o aplicativo saiba onde solicitar os vídeos para cada face e preencher as variáveis de escopo correspondentes; openJSON(). 
	\item Renderizar o ambiente 3D para criar os objetos para cada face nos quais o vídeo será reproduzido; aframe_init()
	\item Carregar os vídeos em cada face, iniciando a reprodução quando o buffer de cada face estiver completamente carregado. initial()
\end{itemize}

MPD de um ladrilho

colocar um print do MPD

Cada player também é configurado com um algoritmo ABR chamado pelo DASH.js de CustomRule do tipo 'qualitySwitchRules'. Este algoritmo é um objeto que é passado por fallback para um agendador do controlador ABR que verifica se é necessário mudar a qualidade e periodicamente. Assim, o controlador ABR chama a função getMaxIndex sempre que o buffer estiver abaixo de um certo nível e fica chamando ela até o buffer atingir o valor máximo. Esta função retorna qual a qualidade/representação (index) que ele deve baixar. Se o buffer estiver sempre cheio ele vai chamar essa função uma vez por gop.

O player usa um arquivo Json para descrição espacial em vez de usar o DASH SRD. Este json possui as configurações necessários para construir o ambiente virtual e as URLs de cada ladrilho.



...

daqui pra baixo o documento do word detalha o software do gabriel

\subsection{Identificador de face}


\subsection{Preditor de Viewport}

Devido a natureza do codificador, a ordem de decodificação dos quadros não é a mesma ordem de reprodução. Assim um chunk necessita ser completamente decodificado para que um quadro arbitrário possa ser acessado e a mudança de qualidade só poderá ocorrer após o término do chunk anterior.

Além disso, como o usuário está em movimento, é necessário prever quais tiles serão visualizados durante toda a reprodução do chunk. Como a projeção deforma a esfera, o número de ladrilhos requisitados varia dependendo de onde a pessoa está olhando.


\subsection{Sobre a estrutura do programa}

\section{Modelagem de QoE}

Tem uma árvore que mostra os elementos de qualidade de algum artigo sobre qoe.


